{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-12T14:51:42.251317Z",
     "start_time": "2021-07-12T14:51:41.358783Z"
    }
   },
   "outputs": [],
   "source": [
    "# importing BeautifulSoup, requests and Pandas\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import concurrent.futures\n",
    "import time\n",
    "import random\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the purpose of demonstration, lets assume we want to scrape some pages from swiggy and get ratings for\n",
    "# the restraunts. As such, we will also assume that we already have the urls for the pages that we want to scrape\n",
    "data_ws = pd.DataFrame(\"Pageurl\" : [\"https://swiggy.com/page1\",\n",
    "                                    \"https://swiggy.com/page2\",\n",
    "                                    \"https://swiggy.com/page3\"]) # list of urls you want to scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-04T21:03:20.750839Z",
     "start_time": "2023-04-04T21:03:20.743672Z"
    }
   },
   "outputs": [],
   "source": [
    "len(\"https://swiggy.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-12T14:51:43.987654Z",
     "start_time": "2021-07-12T14:51:43.888619Z"
    }
   },
   "outputs": [],
   "source": [
    "data_ws.Pageurl = data_ws.Pageurl.apply(lambda x : None if len(x) < 19 else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-12T14:51:44.103627Z",
     "start_time": "2021-07-12T14:51:43.988888Z"
    }
   },
   "outputs": [],
   "source": [
    "data_ws.dropna(how='any',subset=['Pageurl'],inplace=True)\n",
    "data_ws.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-12T14:51:44.236381Z",
     "start_time": "2021-07-12T14:51:44.190957Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dividing a long list into smaller lists of size n\n",
    "def divide_chunks(l, n):\n",
    "    # looping till length l\n",
    "    for i in range(0, len(l), n): \n",
    "        yield l[i:i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-12T14:51:44.323102Z",
     "start_time": "2021-07-12T14:51:44.237119Z"
    }
   },
   "outputs": [],
   "source": [
    "'''Defining Functions to make use of concurrent.futures'''\n",
    "\n",
    "MAX_THREADS = 1 # Depending on the requirement and website limitations\n",
    "\n",
    "\n",
    "def download_url(url):\n",
    "    try:\n",
    "        page_response = requests.get(url,timeout=5)                                      # make a request to the website\n",
    "        page_soup = bs(page_response.text,'html.parser')                       # parse the html link\n",
    "        page_ratings = page_soup.find_all('div',class_='_2l3H5')[0].get_text() # import the value\n",
    "        rest_name = page_soup.find_all('h1',class_='_3aqeL')[0].get_text()\n",
    "    except Exception as e: # handling exceptions is important\n",
    "        page_ratings = '--'\n",
    "        rest_name = '--'\n",
    "    \n",
    "    print(url,' : ',page_ratings, ' : ', rest_name)\n",
    "    x.append(url)\n",
    "    y.append(page_ratings)\n",
    "    z.append(rest_name)\n",
    "        \n",
    "def download_ratings(rating_urls): # we can use concurrency as we dont compute anything, only download things\n",
    "    threads = min(MAX_THREADS, len(rating_urls))\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=threads) as executor:\n",
    "        executor.map(download_url, rating_urls)\n",
    "\n",
    "def main(dataframe):\n",
    "    print('program started at - {}'.format(datetime.datetime.now()))\n",
    "    rating_urls=dataframe.loc[:,'Pageurl']\n",
    "    list_url = list(divide_chunks(rating_urls, 400))\n",
    "    print(len(list_url))\n",
    "    for i in np.arange(0,len(list_url)):\n",
    "        download_ratings(list_url[i])\n",
    "        print('Ratings Done: {}'.format((i+1)*400))\n",
    "        if i < len(list_url)-1:\n",
    "            sleep_time = random.randint(360,480) # adding a random sleep will help lower the stress on website you are scraping\n",
    "            print('Iteration completed at - {}'.format(datetime.datetime.now()))\n",
    "            print('Halting for {} mins'.format(np.round(sleep_time/60,1)))\n",
    "            print('Next iteration will commence at - {}'.format(datetime.datetime.now() + datetime.timedelta(seconds = sleep_time)))\n",
    "        else:\n",
    "            print('Iteration completed at - {}'.format(datetime.datetime.now()))\n",
    "            sleep_time = 0\n",
    "        time.sleep(sleep_time)\n",
    "    print('finished')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-12T15:58:16.974758Z",
     "start_time": "2021-07-12T14:51:44.325466Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x=[]               # to hold urls\n",
    "y=[]               # to hold ratings\n",
    "z=[]\n",
    "main(data_ws)      # calling main to calculate ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-12T15:58:16.990714Z",
     "start_time": "2021-07-12T15:58:16.975709Z"
    }
   },
   "outputs": [],
   "source": [
    "''' Output Dataframe with only pageurls and rating '''\n",
    "out_p=pd.DataFrame(data={'Page Urls' : x,\n",
    "                         'Ratings' : y,\n",
    "                         'Restraunt Name' : z})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-12T15:58:17.133380Z",
     "start_time": "2021-07-12T15:58:16.992664Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "out_p.to_csv(f\"{file_path}.csv\",index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For an indepth explanation of process, you can refer the following articles:  </br>                                 1. https://www.geeksforgeeks.org/implementing-web-scraping-python-beautiful-soup/                                \n",
    "2. https://beckernick.github.io/faster-web-scraping-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
